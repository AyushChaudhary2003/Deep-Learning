{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1xOQfQEi5WkzCMerkekcd7QGWLbAj6FNA","authorship_tag":"ABX9TyPyp8a+xA2rImzfIvIhzDC7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zTCKmQKXlxw8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738156002288,"user_tz":-330,"elapsed":7109,"user":{"displayName":"Ayush Chaudhary","userId":"01347666575785545597"}},"outputId":"e3a51894-7cfa-440b-ceec-8c6402a50c69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Extraction completed!\n","Images Shape: (10000, 784)\n","Labels Shape: (10000,)\n","Labels Shape (One-Hot): (10000, 10)\n","Epoch [1/10], Loss: 35.9943\n","Epoch [2/10], Loss: 35.2796\n","Epoch [3/10], Loss: 33.0666\n","Epoch [4/10], Loss: 28.2009\n","Epoch [5/10], Loss: 21.8599\n","Epoch [6/10], Loss: 16.9217\n","Epoch [7/10], Loss: 13.7153\n","Epoch [8/10], Loss: 11.6469\n","Epoch [9/10], Loss: 10.2780\n","Epoch [10/10], Loss: 9.2510\n","Test Accuracy: 85.80%\n"]}],"source":["#Extract the ZIP File\n","import zipfile\n","import os\n","\n","zip_path = \"/content/drive/MyDrive/DL_Exp/Exp1/archive.zip\"\n","extract_path = \"/content/mnist_data\"\n","\n","if not os.path.exists(extract_path):\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_path)\n","\n","print(\"Extraction completed!\")\n","\n","#check the extracted files:\n","os.listdir(extract_path)\n","\n","#Load MNIST Data Using idx2numpy\n","import idx2numpy\n","import numpy as np\n","\n","image_path = \"/content/mnist_data/t10k-images.idx3-ubyte\"  # Update path if needed\n","label_path = \"/content/mnist_data/t10k-labels.idx1-ubyte\"\n","\n","images = idx2numpy.convert_from_file(image_path)  # Shape: (10000, 28, 28)\n","labels = idx2numpy.convert_from_file(label_path)  # Shape: (10000,)\n","\n","# Normalize pixel values to [0, 1]\n","images = images.reshape(images.shape[0], -1) / 255.0  # Flatten to (10000, 784)\n","\n","print(f\"Images Shape: {images.shape}\")  # (10000, 784)\n","print(f\"Labels Shape: {labels.shape}\")  # (10000,)\n","\n","# One-Hot Encode Labels\n","def one_hot_encode(labels, num_classes=10):\n","    one_hot = np.zeros((labels.size, num_classes))\n","    one_hot[np.arange(labels.size), labels] = 1\n","    return one_hot\n","\n","labels = one_hot_encode(labels)\n","print(f\"Labels Shape (One-Hot): {labels.shape}\")  # (10000, 10)\n","\n","#Initialize Neural Network Parameters\n","#We will build a simple 2-layer neural network:\n","\n","#Input: 784 neurons (flattened 28Ã—28 image)\n","#Hidden Layer: 128 neurons (ReLU activation)\n","#Output: 10 neurons (Softmax for classification)\n","np.random.seed(42)\n","\n","# Initialize weights and biases\n","input_size = 784\n","hidden_size = 128\n","output_size = 10\n","\n","W1 = np.random.randn(input_size, hidden_size) * 0.01\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size) * 0.01\n","b2 = np.zeros((1, output_size))\n","\n","# Define Activation Functions\n","def relu(Z):\n","    return np.maximum(0, Z)\n","\n","def softmax(Z):\n","    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Prevent overflow\n","    return expZ / np.sum(expZ, axis=1, keepdims=True)\n","\n","def relu_derivative(Z):\n","    return Z > 0\n","\n","# Implement Forward & Backward Propagation\n","def forward_propagation(X):\n","    Z1 = np.dot(X, W1) + b1\n","    A1 = relu(Z1)\n","    Z2 = np.dot(A1, W2) + b2\n","    A2 = softmax(Z2)\n","    return Z1, A1, Z2, A2\n","\n","def backward_propagation(X, Y, Z1, A1, Z2, A2, learning_rate):\n","    global W1, b1, W2, b2\n","\n","    m = X.shape[0]  # Number of samples\n","\n","    # Compute gradients\n","    dZ2 = A2 - Y\n","    dW2 = np.dot(A1.T, dZ2) / m\n","    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n","\n","    dA1 = np.dot(dZ2, W2.T)\n","    dZ1 = dA1 * relu_derivative(Z1)\n","    dW1 = np.dot(X.T, dZ1) / m\n","    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n","\n","    # Update parameters\n","    W1 -= learning_rate * dW1\n","    b1 -= learning_rate * db1\n","    W2 -= learning_rate * dW2\n","    b2 -= learning_rate * db2\n","\n","# Train the Model\n","epochs = 10\n","batch_size = 64\n","learning_rate = 0.01\n","\n","for epoch in range(epochs):\n","    total_loss = 0\n","\n","    # Shuffle dataset\n","    indices = np.arange(images.shape[0])\n","    np.random.shuffle(indices)\n","    images, labels = images[indices], labels[indices]\n","\n","    # Mini-batch training\n","    for i in range(0, images.shape[0], batch_size):\n","        X_batch = images[i:i+batch_size]\n","        Y_batch = labels[i:i+batch_size]\n","\n","        Z1, A1, Z2, A2 = forward_propagation(X_batch)\n","        loss = -np.mean(Y_batch * np.log(A2 + 1e-8))  # Cross-entropy loss\n","        backward_propagation(X_batch, Y_batch, Z1, A1, Z2, A2, learning_rate)\n","\n","        total_loss += loss\n","\n","    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n","\n","# Evaluate the Model\n","def predict(X):\n","    _, _, _, A2 = forward_propagation(X)\n","    return np.argmax(A2, axis=1)\n","\n","predictions = predict(images)\n","true_labels = np.argmax(labels, axis=1)\n","\n","accuracy = np.mean(predictions == true_labels) * 100\n","print(f\"Test Accuracy: {accuracy:.2f}%\")\n","\n","\n"]}]}